modularizing
    make a dir: cdcphillib
    the meta thing is called like: imglibraryscraper


live site is missing a bunch of lores and hires images
    for example, id 978
    991

have the mirror site tell you when you're looking at a page that we were unable to scrape

link to the mirror site source code
gpl the mirror site source code

images should have alt text

we should host a tar of the whole dataset

more comprehensive documentation of everything

uploads
* archive.org
* wikimedia commons
    http://commons.wikimedia.org/wiki/Commons:Batch_uploading


categories should be linkified in useful ways

hook in ian's code for some additional metadata


ability to run our re_parse_all_metadata script function
* start.py takes command-line args, maybe


* bug: i think that the image downloader doesn't report success at the end. it seems to hang. maybe one of the threads is stalled?
** a few trials: at the end of a super long scrape it seems to hang. however, once it's restarted after that, it finished just fine.
** yeah, i've had 2 successful runs with small ranges of images to download (5 and 10, i think)

* mirror site should notice where there are holes and store in the db info about what the next viable id is and what the prev viable id is
** this way we don't have to use our hackey workaround to deal with holes in the database

* include a contact email and maybe names on the site.




server:
    get database and images on there
    wordpress on dreamhost
        installed
        parker needs access to the database
        admin accounts
    second wordpress install for our press release/contact us site
    set up database for phil scrape data
    copy phil scrape data
        move it to sql
    check in the theme stuff that basically does all of the image presentation
    maybe deploy out of git


all actions:

dump our data in places
    figure out how to upload to archive.org
        technical task, email task
    figure out how to upload to wikicommons
        technical task, email task
    also flickr commons

writing
    website
        press release
        about us in general
        about this project specifically
    cdc
        polite but declarative
    soliciting funding
        hesperion
        partners in health
            use parker's connection with dr kim
        open data groups?
    solicit advice/tell ppl what we're doing
        carl malahmud (sp?)
        free culture people (discuss@?)
        asheesh
        aaron swartz
        alexis anaynay (bananamana fo fana)





code::::::
move the data into subfolder data/

README
    explain things about the database
        the format for "links" field
        the format for the categories field
        the fact that we have single quotes in places

better var names
hook in ian's stuff
    add metadata that isn't reported but is accessible from search
    image analytics (color/not, size, etc)
parser is smarter--
    knows if we're looking at the page for an id that doesn't exist
handle the case where we get id collisions
    for metadata scraping--we already handle this well for image download
    maybe we just copy our method from the image downloading code
code to find highest index on their db
    search for space char, parse out first result from their results page
parser: better representation of categories?

testing suite type stuff
code to step through the db and see where we have holes.
    also through the file structure
code to take our list of things that failed before, and make sure that they're all empty things.

someday, maybe:
why isn't anyone else doing this?
pharmaceutical companies


analytics to get:
number of images for each year
number of "gaps" in the database
    because if they take something out, it's gone forever
number of illustrations
number of images
number of images
average size of image
number of images that are in the public domain

analytics that we have now:
copyright:
    10506 none
    420 yes copyright


deprecated:
------------------------------------------------------------
cdc scraping notes:

code
    where we are
        parse and scrape work fine
    where we're going
        indexer function
            watching the database for inserts
            watching file downloads
            knows how to restart if things die
        database read
        check_hires
            check if we've downloaded the hires and lores versions of a file
        smarter large-download code
        flag that's like "list what you have"
        flag that's like "fill in the gaps"
        parser should be smarter about grabbing categories
        publish to twitter
    
upload
    wikicommons
        1) pywikipediabot to upload
        2) ability to update with new canonicalized source
        ian will help
    flickr commons
        not until we're official, says seth
        email to cdc first
        definitealy part of the flickr commons project
    archive.org?
    raw dump--
        pirate bay
        legaltorrents
        archive.org
hitting up people for money--also just to let them know what we're doing
putting up a webpage somewhere.  some sort of press release
    name for us
abstracting our code

